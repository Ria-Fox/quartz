# HuggingFace GPU Quantization

GPU Quantizationì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³  ì‹¤í–‰ ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•˜ëŠ” ê¸°ìˆ ë¡œ, ì´ë¥¼ í†µí•´ GPUì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ì†ìƒì‹œí‚¤ì§€ ì•Šìœ¼ë©´ì„œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ë° ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” Int8 í˜¼í•© ì •ë°€ë„ í–‰ë ¬ ë¶„í•´ë¡œ ì„±ëŠ¥ì„ ê±°ì˜ ë–¨ì–´íŠ¸ë¦¬ì§€ ì•Šìœ¼ë©´ì„œ ì†ë„ì™€ ë©”ëª¨ë¦¬ë¥¼ ì•„ë¼ëŠ” ë°©ë²•ì— ëŒ€í•´ ì†Œê°œí•©ë‹ˆë‹¤.

reference : [https://huggingface.co/docs/transformers/ko/perf_infer_gpu_one](https://huggingface.co/docs/transformers/ko/perf_infer_gpu_one)

ë…¸íŠ¸ë¶ : [http://172.16.10.201:8889/notebooks/transformers_int8_quantization.ipynb](http://172.16.10.201:8889/notebooks/transformers_int8_quantization.ipynb) / fd56b464fb94d37cfd55975e2cb300c2a7e50b82d83d550b

<aside>
ğŸ’¡ ì»¤ë„ì€ GPU ì „ìš©ìœ¼ë¡œ ì»´íŒŒì¼ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— í•´ë‹¹ ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•  ê²½ìš° GPUê°€ í•„ìš”í•©ë‹ˆë‹¤. GPUê°€ ì•„ë‹Œ CPUì—ì„œì˜ ìµœì í™”ë¥¼ ì›í•˜ì‹œëŠ” ê²½ìš°, [ONNX ë¦¬ì„œì¹˜](https://www.notion.so/ONNX-2342c57df3624dc1b573430530e2ed97?pvs=21) ë¬¸ì„œë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

</aside>

## 0. ì˜ì¡´ì„±

bitsandbytesì™€ accelerate íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.

```bash
pip install bitsandbytes>=0.39.0 git+https://github.com/huggingface/accelerate.git
```

## 1. ëª¨ë¸ ë¡œë“œ

ì˜ì¡´ì„± ì„¤ì¹˜ í›„, ëª¨ë¸ì„ ë¡œë“œí•  ë•Œ `load_in_8bit=True` ì˜µì…˜ì„ ì„¤ì •í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. í•´ë‹¹ ë¬¸ì„œì—ì„œëŠ” pko-T5-large ëª¨ë¸ë¡œ fine-tuningëœ name-query-relevant ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained(model_folder)
model = AutoModelForSeq2SeqLM.from_pretrained(model_folder, device_map="auto", load_in_8bit=True)
```

## 2. ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤

í•´ë‹¹ quantizationì˜ ê²½ìš° ì¸í¼ëŸ°ìŠ¤ ì‹œ ìœ ì˜ì‚¬í•­ì´ ëª‡ ê°€ì§€ ìˆìŠµë‹ˆë‹¤.

- `pipeline()` í•¨ìˆ˜ ëŒ€ì‹  ëª¨ë¸ì˜ `generate()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. `pipeline()` í•¨ìˆ˜ë¡œë„ ì¶”ë¡ ì€ ê°€ëŠ¥í•˜ë‚˜ í˜¼í•© 8ë¹„íŠ¸ ëª¨ë¸ì— ìµœì í™”ë˜ì§€ ì•Šì•„ ë” ëŠë¦½ë‹ˆë‹¤. ë˜í•œ ì¼ë¶€ ìƒ˜í”Œë§ ì „ëµì€ `pipeline()` í•¨ìˆ˜ì—ì„œëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì…ë ¥ì„ ëª¨ë¸ê³¼ ë™ì¼í•œ GPUì— ë°°ì¹˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

```python
model_inputs = tokenizer(inps, max_length=src_max, padding=True, truncation=True, return_tensors='pt').to(torch.device('cuda'))
preds = model.generate(model_inputs['input_ids'],max_length=out_max)
out = tokenizer.batch_decode(preds, skip_special_tokens=True)
```

## 3. Quantization ê²°ê³¼

ëª¨ë¸ ë¡œë”© RAM : 8000MB â†’ **4000MB**

GPU VRAM : (batch_size=32) 4000MB â†’ **3000MB**

inference ì†ë„ : (batch_size=128) 0.47s â†’ **0.35s**